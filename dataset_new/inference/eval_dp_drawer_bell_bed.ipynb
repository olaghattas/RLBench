{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/franka_deoxys/miniconda3/envs/robodiff/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "pybullet build time: Nov 28 2023 23:52:03\n",
      "/home/franka_deoxys/miniconda3/envs/robodiff/lib/python3.9/site-packages/scipy/__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "import dill\n",
    "import numpy as np\n",
    "import collections\n",
    "import imageio\n",
    "import cv2\n",
    "\n",
    "# -------------------------------\n",
    "# Deoxys and Franka Interface Imports\n",
    "# -------------------------------\n",
    "sys.path.append(\"/home/franka_deoxys/deoxys_control/deoxys\")\n",
    "from deoxys import config_root\n",
    "from deoxys.franka_interface import FrankaInterface\n",
    "from deoxys.utils import YamlConfig\n",
    "from deoxys.utils.config_utils import robot_config_parse_args\n",
    "from deoxys.utils.input_utils import input2action\n",
    "from deoxys.utils.io_devices import SpaceMouse\n",
    "from deoxys.utils.log_utils import get_deoxys_example_logger\n",
    "from deoxys.experimental.motion_utils import follow_joint_traj, reset_joints_to\n",
    "\n",
    "sys.path.append(\"/home/franka_deoxys/deoxys_vision\")\n",
    "from deoxys_vision.networking.camera_redis_interface import CameraRedisSubInterface\n",
    "from deoxys_vision.utils.camera_utils import assert_camera_ref_convention, get_camera_info\n",
    "\n",
    "# -------------------------------\n",
    "# Diffusion Policy Imports\n",
    "# -------------------------------\n",
    "sys.path.append(\"/home/franka_deoxys/diffusion_policy\")\n",
    "from diffusion_policy.workspace.train_diffusion_unet_hybrid_workspace import TrainDiffusionUnetHybridWorkspace\n",
    "from diffusion_policy.model.common.rotation_transformer import RotationTransformer\n",
    "from diffusion_policy.common.pytorch_util import dict_apply\n",
    "from util_eval import RobotStateRawObsDictGenerator, FrameStackForTrans\n",
    "\n",
    "# -------------------------------\n",
    "# Set device\n",
    "# -------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------------\n",
    "# Load Robot and Controller Configurations\n",
    "# -------------------------------\n",
    "args = robot_config_parse_args()\n",
    "robot_interface = FrankaInterface(os.path.join(config_root, args.interface_cfg))\n",
    "controller_cfg = YamlConfig(os.path.join(config_root, args.controller_cfg)).as_easydict()\n",
    "controller_type = args.controller_type\n",
    "\n",
    "# Initialize SpaceMouse (if used)\n",
    "# # For wireless or different devices, adjust vendor/product ids.\n",
    "# spacemouse = SpaceMouse(vendor_id=9583, product_id=50770)\n",
    "# spacemouse.start_control()\n",
    "\n",
    "# (Assume that RobotStateRawObsDictGenerator is available in your environment)\n",
    "# For example:\n",
    "# from deoxys.raw_obs_generator import RobotStateRawObsDictGenerator\n",
    "raw_obs_dict_generator = RobotStateRawObsDictGenerator()\n",
    "\n",
    "def set_gripper(open=True):\n",
    "    d = -1.0 if open else 1.0\n",
    "    action_close = np.array([0., 0., 0., 0., 0., 0., d])\n",
    "    robot_interface.control(\n",
    "        controller_type=controller_type,\n",
    "        action=action_close,\n",
    "        controller_cfg=controller_cfg,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Camera Info for rs_0: {'camera_id': 0, 'camera_type': 'rs', 'camera_name': 'camera_rs_0'}\n",
      "CameraRedisSubInterface:: {'camera_id': 0, 'camera_type': 'rs', 'camera_name': 'camera_rs_0'} True False\n",
      "Camera Info for rs_1: {'camera_id': 1, 'camera_type': 'rs', 'camera_name': 'camera_rs_1'}\n",
      "CameraRedisSubInterface:: {'camera_id': 1, 'camera_type': 'rs', 'camera_name': 'camera_rs_1'} True False\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Setup Camera Interfaces\n",
    "# -------------------------------\n",
    "camera_ids = [0, 1]\n",
    "cr_interfaces = {}\n",
    "use_depth = False\n",
    "\n",
    "for camera_id in camera_ids:\n",
    "    camera_ref = f\"rs_{camera_id}\"\n",
    "    assert_camera_ref_convention(camera_ref)\n",
    "    camera_info = get_camera_info(camera_ref)\n",
    "    print(\"Camera Info for {}:\".format(camera_ref), camera_info)\n",
    "    cr_interface = CameraRedisSubInterface(camera_info=camera_info, use_depth=use_depth, redis_host='127.0.0.1')\n",
    "    cr_interface.start()\n",
    "    cr_interfaces[camera_id] = cr_interface\n",
    "\n",
    "def get_imgs(use_depth=False):\n",
    "    data = {}\n",
    "    for camera_id in camera_ids:\n",
    "        img_info = cr_interfaces[camera_id].get_img_info()\n",
    "        data[f\"camera_{camera_id}\"] = img_info\n",
    "\n",
    "        imgs = cr_interfaces[camera_id].get_img()\n",
    "        color_img = imgs[\"color\"][..., ::-1]  # Convert from BGR to RGB if needed.\n",
    "        color_img = cv2.resize(color_img, (320, 240))\n",
    "        data[f\"camera_{camera_id}_color\"] = color_img\n",
    "\n",
    "        if use_depth:\n",
    "            depth_img = imgs[\"depth\"]\n",
    "            depth_img = cv2.resize(depth_img, (224, 224))\n",
    "            data[f\"camera_{camera_id}_depth\"] = depth_img\n",
    "    return data\n",
    "\n",
    "def get_current_obs():\n",
    "    \"\"\"\n",
    "    Gather the latest robot state and images to form an observation dictionary.\n",
    "    The raw state is obtained from the deoxys robot interface, while images come from the camera interfaces.\n",
    "    \"\"\"\n",
    "    last_state = robot_interface._state_buffer[-1]\n",
    "    last_gripper_state = robot_interface._gripper_state_buffer[-1]\n",
    "    obs_dict = raw_obs_dict_generator.get_raw_obs_dict({\n",
    "        \"last_state\": last_state,\n",
    "        \"last_gripper_state\": last_gripper_state\n",
    "    })\n",
    "    data = get_imgs(use_depth=False)\n",
    "    # Map your camera images to the keys expected by the diffusion policy.\n",
    "    # Here, we assume:\n",
    "    #   agentview_image  <- wrist camera image from camera_0_color\n",
    "    #   robot0_eye_in_hand_image  <- front camera image from camera_1_color\n",
    "    agentview_img = data['camera_0_color']\n",
    "    eye_in_hand_img = data['camera_1_color']\n",
    "    # Transpose images to channel-first if required.\n",
    "    obs_dict['agentview_rgb'] = agentview_img.transpose(2, 0, 1).astype(np.float32) / 255.0\n",
    "    obs_dict['eye_in_hand_rgb'] = eye_in_hand_img.transpose(2, 0, 1).astype(np.float32) / 255.0\n",
    "    return obs_dict\n",
    "\n",
    "# -------------------------------\n",
    "# Helper Classes\n",
    "# -------------------------------\n",
    "class FrameStackForTrans:\n",
    "    def __init__(self, num_frames):\n",
    "        self.num_frames = num_frames\n",
    "        self.obs_history = {}\n",
    "\n",
    "    def reset(self, init_obs):\n",
    "        self.obs_history = {}\n",
    "        for k in init_obs:\n",
    "            self.obs_history[k] = collections.deque([init_obs[k][None] for _ in range(self.num_frames)], maxlen=self.num_frames)\n",
    "        return {k: np.concatenate(self.obs_history[k], axis=0) for k in self.obs_history}\n",
    "\n",
    "    def add_new_obs(self, new_obs):\n",
    "        for k in new_obs:\n",
    "            if 'timesteps' in k or 'actions' in k:\n",
    "                continue\n",
    "            self.obs_history[k].append(new_obs[k][None])\n",
    "        return {k: np.concatenate(self.obs_history[k], axis=0) for k in self.obs_history}\n",
    "\n",
    "def undo_transform_action(action, rotation_transformer):\n",
    "    # This mirrors your simulation code logic.\n",
    "    raw_shape = action.shape\n",
    "    if raw_shape[-1] == 20:\n",
    "        action = action.reshape(-1, 2, 10)\n",
    "    d_rot = action.shape[-1] - 4\n",
    "    pos = action[..., :3]\n",
    "    rot = action[..., 3:3+d_rot]\n",
    "    gripper = action[..., -1:]\n",
    "    rot = rotation_transformer.inverse(rot)\n",
    "    uaction = np.concatenate([pos, rot, gripper], axis=-1)\n",
    "    if raw_shape[-1] == 20:\n",
    "        uaction = uaction.reshape(*raw_shape[:-1], 14)\n",
    "    return uaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============= Initialized Observation Utils with Obs Spec =============\n",
      "\n",
      "using obs modality: low_dim with keys: ['ee_states', 'gripper_states', 'joint_states']\n",
      "using obs modality: rgb with keys: ['agentview_rgb', 'eye_in_hand_rgb']\n",
      "using obs modality: depth with keys: []\n",
      "using obs modality: scan with keys: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/franka_deoxys/miniconda3/envs/robodiff/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/franka_deoxys/miniconda3/envs/robodiff/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diffusion params: 2.564722e+08\n",
      "Vision params: 2.239418e+07\n",
      "Diffusion policy loaded and set to eval mode.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Load Diffusion Policy Checkpoint\n",
    "# -------------------------------\n",
    "\n",
    "checkpoint = \"/home/franka_deoxys/data_franka/epoch_700_drawer_bellpepper_bed.ckpt\"\n",
    "with open(checkpoint, 'rb') as f:\n",
    "    payload = torch.load(f, pickle_module=dill)\n",
    "cfg = payload['cfg']\n",
    "\n",
    "workspace = TrainDiffusionUnetHybridWorkspace(cfg, output_dir=None)\n",
    "workspace.load_payload(payload, exclude_keys=None, include_keys=None)\n",
    "\n",
    "# Select the policy model (use EMA model if enabled)\n",
    "policy = workspace.model\n",
    "if getattr(cfg.training, \"use_ema\", False):\n",
    "    policy = workspace.ema_model\n",
    "policy.to(device)\n",
    "policy.eval()\n",
    "print(\"Diffusion policy loaded and set to eval mode.\")\n",
    "\n",
    "# -------------------------------\n",
    "# Optional: Initialize Rotation Transformer (if using absolute actions)\n",
    "# -------------------------------\n",
    "abs_action = True\n",
    "rotation_transformer = None\n",
    "if abs_action:\n",
    "    rotation_transformer = RotationTransformer('axis_angle', 'rotation_6d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Inference / Rollout Function\n",
    "# -------------------------------\n",
    "def rollout_diffusion(policy, rotation_transformer, n_obs_steps, n_action_steps, max_steps, return_imgs=False):\n",
    "    \"\"\"\n",
    "    This function repeatedly obtains observations from the robot,\n",
    "    uses the diffusion policy to predict an action, and then sends the action\n",
    "    to the Franka via the deoxys interface.\n",
    "    \"\"\"\n",
    "    # Keys expected by the policy (images and robot state).\n",
    "    # keys_select = ['joint_states', 'ee_states', 'eye_in_hand_rgb', 'gripper_states']\n",
    "    keys_select = ['agentview_rgb', 'joint_states', 'ee_states', 'eye_in_hand_rgb', 'gripper_states']\n",
    "    imgs = []\n",
    "    framestacker = FrameStackForTrans(n_obs_steps)\n",
    "    obs = get_current_obs()\n",
    "    # (Mapping: ensure that any additional keys required by your model are present.)\n",
    "    obs = framestacker.reset(obs)\n",
    "    done = False\n",
    "    success = False\n",
    "    step = 0\n",
    "\n",
    "    while not done and step < max_steps:\n",
    "        # Prepare the observation dictionary for the policy.\n",
    "        np_obs_dict = {key: obs[key][None, :] for key in keys_select if key in obs}\n",
    "        obs_tensor = dict_apply(np_obs_dict, lambda x: torch.from_numpy(x).to(device))\n",
    "        with torch.no_grad():\n",
    "            action_dict = policy.predict_action(obs_tensor)\n",
    "        np_action_dict = dict_apply(action_dict, lambda x: x.detach().cpu().numpy())\n",
    "        env_action = np_action_dict['action']\n",
    "        env_action = undo_transform_action(env_action, rotation_transformer)\n",
    "        env_action = env_action.squeeze()\n",
    "        obs_img = get_current_obs()\n",
    "        # print(obs_img['eye_in_hand_rgb'].shape)\n",
    "        # For safety, you might use only a subset of the predicted action.\n",
    "        for delta_action in env_action[:4]:  # adjust slicing as required by your controller\n",
    "        # print(delta_action)\n",
    "\n",
    "            # Send control command to the robot.\n",
    "            robot_interface.control(\n",
    "                controller_type=controller_type,\n",
    "                action=delta_action,  # Convert numpy array to list of floats\n",
    "                controller_cfg=controller_cfg,\n",
    "            )\n",
    "\n",
    "\n",
    "        # Optionally, capture images after applying the action.\n",
    "        if return_imgs:\n",
    "            obs_img = get_current_obs()\n",
    "            # For visualization, record one of the camera images.\n",
    "            imgs.append(obs_img['eye_in_hand_rgb'].transpose(1, 2, 0))  # convert to HxWxC\n",
    "\n",
    "        step += 1\n",
    "        # A simple termination condition: you can add your own task success logic.\n",
    "        if step >= max_steps:\n",
    "            done = True\n",
    "        else:\n",
    "            # Update observation with new sensor data.\n",
    "            new_obs = get_current_obs()\n",
    "            obs = framestacker.add_new_obs(new_obs)\n",
    "            # Optionally, set a success flag if a condition is met.\n",
    "            # success = check_success_condition(new_obs)\n",
    "\n",
    "    return success, imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# block normal\n",
    "# reset_joint_positions_d = [-0.03,0.244,-0.03,-1.673,0.134,1.905,0.665]\n",
    "# block agent plus eye\n",
    "# reset_joint_positions_b= [0.034,0.109,-0.012,-1.63,0.005,1.776,0.696]\n",
    "reset_joint_positions = [\n",
    "    0.09162008114028396,\n",
    "    -0.19826458111314524,\n",
    "    -0.01990020486871322,\n",
    "    -2.4732269941140346,\n",
    "    -0.01307073642274261,\n",
    "    2.30396583422025,\n",
    "    0.8480939705504309,\n",
    "]\n",
    "# candy\n",
    "# reset_joint_positions_c = [-0.03,0.244,-0.03,\n",
    "#                                -1.673,0.134,1.905,0.665]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # reset robot\n",
    "# set_gripper(open=True)\n",
    "# reset_joints_to(robot_interface, reset_joint_positions)\n",
    "# set_gripper(open=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting trial \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# for trial in range(n_trials):\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting trial \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m success, imgs \u001b[38;5;241m=\u001b[39m \u001b[43mrollout_diffusion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrotation_transformer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_obs_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mn_action_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_imgs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m trial_success\u001b[38;5;241m.\u001b[39mappend(success)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# print(\"Trial {} success: {}\".format(trial+1, success))\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# if imgs:\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m#     video_filename = f\"trial_{trial+1}_output.mp4\"\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m#     imageio.mimwrite(video_filename, imgs, fps=fps, quality=8)\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m#     print(\"Saved video:\", video_filename)\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Pause briefly between trials.\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 27\u001b[0m, in \u001b[0;36mrollout_diffusion\u001b[0;34m(policy, rotation_transformer, n_obs_steps, n_action_steps, max_steps, return_imgs)\u001b[0m\n\u001b[1;32m     25\u001b[0m obs_tensor \u001b[38;5;241m=\u001b[39m dict_apply(np_obs_dict, \u001b[38;5;28;01mlambda\u001b[39;00m x: torch\u001b[38;5;241m.\u001b[39mfrom_numpy(x)\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 27\u001b[0m     action_dict \u001b[38;5;241m=\u001b[39m \u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m np_action_dict \u001b[38;5;241m=\u001b[39m dict_apply(action_dict, \u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m     29\u001b[0m env_action \u001b[38;5;241m=\u001b[39m np_action_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maction\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/diffusion_policy/diffusion_policy/policy/diffusion_unet_hybrid_image_policy.py:258\u001b[0m, in \u001b[0;36mDiffusionUnetHybridImagePolicy.predict_action\u001b[0;34m(self, obs_dict)\u001b[0m\n\u001b[1;32m    255\u001b[0m     cond_mask[:,:To,Da:] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;66;03m# run sampling\u001b[39;00m\n\u001b[0;32m--> 258\u001b[0m nsample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconditional_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcond_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcond_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_cond\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_cond\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m    \u001b[49m\u001b[43mglobal_cond\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mglobal_cond\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# unnormalize prediction\u001b[39;00m\n\u001b[1;32m    266\u001b[0m naction_pred \u001b[38;5;241m=\u001b[39m nsample[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m,:Da]\n",
      "File \u001b[0;32m~/diffusion_policy/diffusion_policy/policy/diffusion_unet_hybrid_image_policy.py:196\u001b[0m, in \u001b[0;36mDiffusionUnetHybridImagePolicy.conditional_sample\u001b[0;34m(self, condition_data, condition_mask, local_cond, global_cond, generator, **kwargs)\u001b[0m\n\u001b[1;32m    192\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mset_timesteps(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_inference_steps)\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m scheduler\u001b[38;5;241m.\u001b[39mtimesteps:\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# 1. apply conditioning\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     trajectory[condition_mask] \u001b[38;5;241m=\u001b[39m \u001b[43mcondition_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcondition_mask\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;66;03m# 2. predict model output\u001b[39;00m\n\u001b[1;32m    199\u001b[0m     model_output \u001b[38;5;241m=\u001b[39m model(trajectory, t, \n\u001b[1;32m    200\u001b[0m         local_cond\u001b[38;5;241m=\u001b[39mlocal_cond, global_cond\u001b[38;5;241m=\u001b[39mglobal_cond)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# policy.reset()\n",
    "# -------------------------------\n",
    "# Main Inference Loop\n",
    "# -------------------------------\n",
    "\n",
    "n_obs_steps = getattr(cfg, \"dataset_obs_steps\", 2)\n",
    "n_action_steps = getattr(cfg, \"n_action_steps\", 4)\n",
    "max_steps = 400   # maximum steps per trial\n",
    "n_trials = 5      # number of inference trials\n",
    "fps = 20          # frames per second for output video\n",
    "\n",
    "trial_success = []\n",
    "# for trial in range(n_trials):\n",
    "print(\"Starting trial \")\n",
    "success, imgs = rollout_diffusion(policy, rotation_transformer, n_obs_steps,\n",
    "                                    n_action_steps, max_steps, return_imgs=True)\n",
    "trial_success.append(success)\n",
    "# print(\"Trial {} success: {}\".format(trial+1, success))\n",
    "# if imgs:\n",
    "#     video_filename = f\"trial_{trial+1}_output.mp4\"\n",
    "#     imageio.mimwrite(video_filename, imgs, fps=fps, quality=8)\n",
    "#     print(\"Saved video:\", video_filename)\n",
    "# Pause briefly between trials.\n",
    "time.sleep(2.0)\n",
    "\n",
    "mean_success = np.mean(trial_success)\n",
    "print(\"Mean success over trials:\", mean_success)\n",
    "\n",
    "# Close robot interface safely.\n",
    "robot_interface.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "robodiff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
